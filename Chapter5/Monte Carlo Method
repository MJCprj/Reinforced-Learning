# 몬테카를로 방식Monte Carlo Method

기존에 배운 Dynamic Programming은 MDP를 알고 있는 경우에 Bellman 방정식으로 풀어내서 GPI (Generalized Policy Iteration)을 따르는 방식으로 최적화 정책을 찾아냈다.

이제는 환경에 대한 정보를 알수 없는,  MDP를 알 수 없는 상황에서 환경과 직접적으로 상호작용하면서 경험을 통해서 학습하는 방식인 **Model-free 방식인 몬테카를로 방식에 대해 설명한다.**

Monte Carlo라는 이름은 모나코의 행정구 중에 하나로 카지노가 유명한 곳이다. 카지노처럼 무작위성을 고려해야한다는 의미를 따 카지노로 유명한 도시 이름을 차용한 방법이다. Monte Carlo 방법은 강화학습 뿐만 아니라, 어떤 모델을 알기 어려울 때나 특성을 알아볼 때 유용한 방법론이다.
예를 들어, 사분원의 넓이를 이용해 pi값을 알고 싶다면 1*1 정사각형에 무작위로 다트를 던진다. 이때, 맞는 곳이 무작위라면 전체 다트 수에서 사분원 안에 들어간 다트수가
(사진)
위 사진과 같게 된다. MC 방법은 MC simulation이라고도 하는데 이와 같이 계속 시뮬레이션을 통해 값을 추정해나갈 수 있기 때문이다.
그러나 이렇게 시뮬레이션을 하다보면, 한계가 드러나게 된다.

- 샘플이 적을 경우
- 샘플이 2차원에 있지 않고 높은 차원에 있을 경우
- 원인이 다양할 경우

그렇기 때문에 몬테카를로 방법은 샘플이 충분히 많고, 낮은 차원에서 샘플링의 확률 분포를 알고 있을때 적용이 가능하다.

**모델 프리란?**

- 보상 함수와 전이 확률을 모르는 상황
- '모델을 모른다', 'MDP를 모른다', '모델 free다'는 같은 의미

## **Model-free에서 Prediction 과 Control**

**Prediction :** 가치(value)를 estimate(추정)하는 것을 말한다. model-free에서 prediction은 환경에 대한 사전 지식이 없는 상태에서 환경과 상호 작용하여 가치 함수(value function)를 estimate(추정)해가는 방식을 의미한다.

**Control :**  Prediction에서 찾은 value function을 최적화하여 최적의 policy 를 찾는 것을 의미한다.

**이러한 Model-free 방식에는**

**`Monte-Carlo Learning(MC)`방식과** 

**`Temporal-Difference(TD)`가 대표적이다.**

## **MC의 특징**

<aside>
💡 **MC(Monte-Carlo)는 샘플링된 에피소드로부터 학습하는 방법이다.**

</aside>

- MC는 Model-free 방식으로 MDP에 대한 transition 와 보상을 모르는 상태에서 시작한다.
- MC(Monte-Carlo)는 샘플링된 에피소드로부터 학습하는 방법이다.
- 상태 가치 $v_{\pi}(s)$는 정책 $\pi$에서 샘플링 된 에피소드로부터 평균 보상으로써 경험하여 학습한다.
- 항상 에피소드가 종료가 되어야 가치를 업데이트할 수 있다.
(에피소드가 완료된 후에 업데이트하기 때문에 수렴 속도가 느리다.)
- 그렇기 때문에 bootstrapping이 아니다.
- episodic 문제에서만 사용할 수 있다.

## DP와의 비교

DP와 비교되는 장점
- 환경과 직접적으로 상호작용하여 최적 행동을 학습하는데 사용될 수 있음
- 시뮬레이션 또는 표본 모델과 같이 사용될 수 있음
- 상태 집합의 작은 부분집합에 대한 몬테카를로 방법에 초점을 두는 것은 쉬우면서도 효율적임(특별히 관심 있는 영역에 대해서만 정확한 평가를 수행하는것이 가능 - 8장에서 다룰 내용)
- 마르코프 특성에 위배되는 상황에서 몬테카를로 방법이 더 강인함 가치 추정값을 갱신할 때 이후상태들에 대한 가치 추정값을 이용하지 않기 때문==부트스트랩 방법이 아니기 때문

Dynamic Programming 에서는 Large model 이 필요했지만 몬테카를로의 경우는 필요한 정책에 해당하는 상태들의 가치만 고려하면 되기 때문에 large model 이 필요하지 않다.

또한 몬테카를로의 연산량은 MDP의 사이즈와는 상관 없이 에피소드의 길이와 연관관계가 있다.

(사진)

에피소드를 진행하게 되면서 지나가게 되는 경로를 Trajectory라고 한다. 즉 $s_1$에서 $a_1$을 취하면 $r_2$를 얻게 되고, 이런 식이 에피소드가 끝날 때까지 연속되게 얻은 것을 의미하기 때문이다.

최종 G를 얻기 위해서 가치 함수(value function)을 정의하게 되는데, 이는 기대된 이득을 말한다. Expected Return(기대된 이득)은 discounted된 모든 보상의 합을 의미한다.

MDP에서 미래에 받을 reward를 현재 가치로 discounted 하여 더한 값을 return이라 정의하고 value 함수는 state s에서 return의 기댓값이 정의했지만, 

model-free 방법에서는 MDP를 모르는 상태이고 그 중 하나의 방법론인 MC에서 Prediction은 하나의 에피소드를 마치고 얻은 return(이득)들의 평균값으로 value function을 근사하고 이를 반복해 true value function을 찾자는 것이다.

(사진)

그림은 임의의 policy에 대해 첫번째 state부터 teriminal state까지를 하나의 에피소드를 나타낸 그림이다. state를 이동할 때마다 얻은 reward들을 통해 각 state별 return 인 $G(s)$를 구할 수 있습니다.

이전 DP에서는 true value function을 구하기 위해서 여러 번의 iteration을 통해 value function을 업데이트했다면,

MC에서는 여러 episode를 진행하여 해당 state를 지나가는 episode에 return을 모두 구해서 산술 평균을 구한다. 이러한 return 값들이 episode 반복을 통해 여러 값들이 모이면 true value function으로 근사할 수 있게 된다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/15620742-9cdb-4719-b5f7-5dba56a47cd4/Untitled.png)

여기서 *N* (s)는 total episode동안 state s를 방문한 횟수, *G* i(s)는 episode i에서 state s의 return값임

## 몬테카를로 예측의 행동가치함수 구하기

(사진)
(사진)
몬테카를로에서 V를 구하는 부분을 Q함수를 구하는 것으로만 변경해주면 된다. State와 Action을 선택할 때의 가치를 구하고, 이를 더해 평균을 취해주면 된다.

## 행동가치함수가 MC에서 중요한 이유

(사진)

행동가치함수는 "행동"에 중점을 둔 가치함수라 "행동"을 결정하는데 더욱 직관적인 함수이다. 반면에 가치함수를 적용하게 된다면, 가치함수는 상태의 가치 그 자체를 판단하기 때문에 가치를 통해 최적의 행동을 결정하기 때문에 "행동"이라는 패턴에 도달하기에 한계가 있다.

따라서 만약에 강화학습 에이전트에 "탐험(무작위 선택)"의 요소를 넣고 싶을 때, 행동가치함수로 하게 되면 간단하지만, 가치함수로 하게 되면 "가치함수 -> 행동 알아내기" 라는 불필요한 연산이 하나 더 추가되기 때문에 연산도 비효율적이고 직관적이지도 않다.

그러므로 행동가치함수는 "탐험"의 요소를 더욱 쉽게 넣기 위해서 중요하게 고려해야 한다.

(사진)

최적의 선택을 하기 위해서는, 특히 학습 초기의 경우에 무작위적인 행동이 들어가는, 탐험을 적절히 넣어주는게 중요하다.

## 몬테카를로 제어

일반화된 정책 반복GPI의 개념을 따라 나아간다. 근사 정책, 근사 가치함수 모두 유지해야 한다.

매번 서로의 목표를 이동시키기 때문에 일정 정도 서로의 목표 달성을 방해하지만, 결국 최적 정책과 최적 가치함수에 도달하게 되는 원리이다.

이를 실제 사용하려면

> NOT 에피소드가 시작 탐험을 가진다
NOT 정책 평가가 무한개의 에피소드에 대해 행해진다
> 

이를 명확하게 이해하기 위해서는 몬테카를로 방식의 수렴성에 대해 알아보아야 한다.

# Monte Carlo의 수렴성

몬테카를로 방법은 모델을 활용할 수 없기에 (model-free) 상태가치함수 대신 action value (state-action value)를 추정하는 것이 유용하다. (또는 둘 다 고려하는것이 유용하다.)

state-action 쌍을 마주치는 것은 하나의 에피소드에서 state s를 마주치고 그 state s에서 action a가 선택되는 것이라고 이해할 수 있다.

여기서 유일한 문제는
많은 state-action 쌍에 대해 접촉이 발생하지 않을 수도 있다는 점인데, policy 𝝅의 결과대로만 action을 선택하면 그에 대한 이득만 확인하게 되어 다른 action에 대한 경험을 얻지 못하기 때문이다. 즉, 현재 선호하는 action뿐만 아니라 각 state로부터 선택할 수 있는 모든 action의 가치를 추정할 필요가 있다.

그러므로

MC 방법의 수렴성, 모든 state-action 쌍을 무한 번 마주치는 것을 쉽게 보장받기 위해서는 있을 법하지 않은 두 가지 가정을 해야한다.

- episode가 시작 탐험 exploring starts을 갖는다
    - **== episode 개수가 무한으로 갈 때 모든 state-action 쌍을 무한 번 마주치는 것을 보장함**
    - action value에 대한 policy evalutation은 **연속적인 탐험**이 보장되어야만 한다(maintaining exploration)
    - episode가 하나의 state-action 쌍에서 시작하고,그 외 다른 쌍들이 시작 쌍으로 선택될 확률이 0이 아님을 명시하는 것
- policy evalution이 무한 개의 episode에 대해 행해질 수 있다

그러나 실제 문제에 적용하기 위해서는 이 두 가지 가정(조건)을 제거해야 한다.

### 우선, 정책 평가를 무한정으로 제공하지 않는다면

- 추정 오차의 크기와 확률이 특정 경계값보다 낮을만큼 수렴되었을 때 중단된다. 하지만 최소 규모의 문제가 아니라면 실제 문제에 유용하게 적용하기 위해 너무 많은 에피소드를 필요로 할 수 있다.
- policy improvement 전에 evaluation을 완료하려는 노력을 포기한다면, 매번의 평가 단계에서 value function은 *qπ*를 '향해' 움직이지만, 많은 단계를 거치기 전에 실제로 가까워질 것이라 기대하지 않는다.

## 시작 탐험을 적용한 몬테카를로 Monte Carlo ES

- 정책과 가치함수가 모두 최적일 때만 Stability가 유지된다고 한다.
(그러나 증명된 것은 없어서 RL의 미해결 문제로 남게 된다.)
- episode마다 평가 → 향상이 번갈아 수행되는게 자연스럽다. 각 episode 이후에 관측된 이득이 정책 평가에 활용되고 나면 episode에서 마주치는 모든 상태에서 정책이 향상될 수 있다.

## 시작 탐험 없는 몬테카를로 제어

모든 행동이 무한히 선택되도록 보장하는 일반적 방법은 에이전트로 하여금 계속해서 행동을 선택하게 하는 것이다. 이를 수행하는 방법은 크게 두 가지로,

1. **활성 정책 on-policy**
2. **비활성 정책 off-policy**

이다.

### on-policy (활성 정책)

- 결정을 내리는데 사용되는 policy를 평가하고 향상시킨다.
- soft (관대한) 편이다.
    - 모든 state와 action에 대해 *π*(*a*∣*s*)>0을 만족하면서
    - 결정론적인 optimal policy에 가깝게 이동한다는 의미
- ex) Monte Calro ES, Epsilon-greedy policy

### off-policy (비활성 정책)

- 자료를 생성하는데 사용되지 않는 policy를 평가하고 향상시킨다(독립적)
- Target policy와 동떨어진 데이터로부터 학습
- 두 가지 policy를 사용한다.
    - target policy : 학습 대상이자 optimal policy
    - behavior policy : 좀 더 탐험적이고 action 생성을 위한 policy
- 서로 다른 policy를 사용하기에 분산이 더 크고 수렴 속도가 느리다
- target policy == behavior policy 인 특별한 경우에 off-policy는 on-policy를 포함한다
- ex) Importance Sampling (중요도 추출법)
- 그러나 모든 행동이 탐욕적일 때 학습이 크게 지연되며 문제가 발생할 수 있다.

### epsilon-greedy policy

on-policy 방법의 일종으로 대부분의 시간동안 최대의 action value 추정값을 갖는 action을 선택하지만 입실론 *ϵ*의 확률로 이따금씩 무작위로 action을 선택하는 것을 의미한다.

### 비활성 정책 추정기의 분산 감소 방법 : Importance Sampling

어떤 분포로부터 얻어진 표본이 주어질 때, 그 표본을 이용하여 또 다른 분포에서의 기댓값을 추정하는 방법이다. target policy와 behavior policy에서 발생하는 state-action의 궤적에 대한 (=*At*,*St*+1,*At*+1...*ST*) 상대적 확률을 Importance Sampling ratio (중요도추출비율)이라 한다. 이 확률에 따라 가중치를 부여하는 방식으로 off-policy에 importance sampling을 적용할 수 있다.

최신 연구 기법에는 두 가지 예시가 적절하다. 이들은 모두 이득의 내부 구조를 사용하여 비활성 정책 추정기의 분산을 감소시키는 기법이다. 

- 할인을 고려한 중요도 추출법 : 할인된 보상의 총합이 아니라 특정한 요소와 그 이득의 가중치를 고려하는 방법. (한 단계 이후에 1-r 정도로 보상의 총합을 통해 예측되는 정책 추정의 분산을 낮추는 방법임)
- 결정 단계별 중요도 추출법 : 할인이 없이도 처음과 마지막 요소만을 고려하여 단계별로 중요도를 추출하는 방식
