
# 선택 문제 Bandit Problem

Bandit Problem은 오직 하나의 상태만 다루는 강화학습 문제이다.

강화 학습은 지도 학습과는 다른 종류의 학습 피드백을 전달한다. 학습자의 행동과 무관하게 정해져 있는 정답이 있는 지도 학습과는 다르게, 강화 학습은 행동에 대한 결과를 피드백으로 전달한다. 그렇기 때문에 좋은 행동을 찾기 위한 직접적인 탐색이 필요하다.

단순화 된 구조에서 하나의 상황에 대해서만 행동을 학습하게 되는 구조를

### **Non-Associative 구조라고 한다.**

- 하나 이상의 상황situation을 다루지 않음.
- 평가에 대한 피드백Evaluative Feedback에 관한 작업이 이미 수행 된다.

이러한 구조를 가지는 특별한 구조로서 다중 선택 문제(Multi-Armed Bandits)가 있다.

# **Multi-Armed Bandits**

MAB는 카지노에 있는 여러 개의 슬롯머신(Bandit) 중 어떤 슬롯 머신의 손잡이(Arm)을 내려야 최적의 수익을 얻을 수 있을지 고민하는데서 시작되었다.
카지노의 고객들은 특정 슬롯 머신의 수익률이 좋다는 사실을 경험적으로 알아내는데, 이때 고객들이 사용하는 전략이 바로 선택 문제라고 할 수 있다.

K-Armed Bandit Problem(다중 선택 문제)는 슬롯 머신과 유사하기 때문에 붙여졌는데

- k개의 서로 다른 옵션이나 행동 중 하나를 계속해서 선택해야 하고
- 선택 후에는 수치화된 보상이 주어진다.
- 선택의 목적은 반복적인 선택을 통한 누적 보상을 최대로 만드는 것이다.

k개의 행동 각각에는 선택 되었을 때 기대 평균 보상 값이 있다. 

평균 보상 값을 가치(Value)라고 하자.

임의의 행동 a의 기대 평균 보상(가치)인 *q*∗(*a*)는 아래의 식으로 표현할 수 있다.

 $q_*(a) \dot{=} E[R_t | A_t = a]$

- *At* : 시간 t에서 선택 되는 행동 ( Action )
- *Rt* : 시간 t에서 행동에 따른 보상 ( Reward )

각 행동에 대한 가치의 확실하지 않다는 전제를 가진다. 시간 t에서 추정한 행동 a의 가치를 *Qt*(*a*)로 표현하는데, 추정값 *Qt*(*a*)가 기댓값*q*∗(*a*)와 가까워질수록 정확한 추정이 된다.

행동에 대한 가치를 시간에 따라 추정하면 각 시간마다 가치가 최대인 행동을 하나 이상 선택할 수 있는데 이러한 방법을 탐욕적 행동(Greedy action)이라고 한다.

### **Greedy Action**

이 방법은 최대의 가치를 가지는 행동을 선택하는 방법이다. 탐욕적 행동을 선택하는 것은 현재 까지 가지고 있는 지식을 최대한 **활용(Exploiting)** 하는데

탐욕적 행동은 단기적으로는 좋을 수 있지만, 그리디 행동의 기반인 가치가 불확실하기 때문에 장기적으로 더 많은 선택을 해야한다면 좋지 않을 수 있다.

비-탐욕적 행동을 하는 것을 **탐험(Exploring)**이라고 하는데 탐험으로 비-탐욕적 행동의 추정 가치를 상승 시킬 수 있다. 탐험으로 더 높은 가치를 가지는 행동을 찾아내고 그 행동을 더 많이 선택함으로써 누적된 보상의 총합을 상승 시킬 수 있기 때문에 장기적으로는 더 큰 보상을 얻을 수 있다. 하지만 행동을 선택할 때 한번에 활용과 탐색을 할 수는 없기 때문에 균형을 맞춰야 한다.

**활용Exploiting**과 **탐험Exploring**을 결정하는 것은 복잡한 방법으로 결정 된다.

- Action에 대한 정밀한 가치 추정값.
- 가치 추정값의 불확실성.
- 앞으로 남아 있는 단계의 수.

# **행동 가치 방법 ( Action-Value Method )**

행동의 가치를 추정하고 추정값으로 부터 행동을 선택하는 결정하는 방법

*첫번째, 행동의 가치 추정*

행동이 가지는 가치는 행동이 선택될 때 받을 수 있는 보상의 평균이다. 평균 보상을 추정하는 방법은 최선의 방식은 아니지만 직관적으로 실제로 받은 보상의 산술평균을 계산함으로 추정 할 수 있다. 이 방법을 **표본 평균법(sample-average)**이라고 하고 아래의 식으로 표현할 수 있다.

 $Q_t(a) \dot{=} \frac{시각 \ t \ 이전에 \ 취해지는 \ 행동 \ a에 \ 대한 \ 보상의 \ 총합}{시각 \ t \ 이전에 \ 행동 \ a를 \ 취하는 \ 횟수 } = \frac{\sum_{i=1}^{t-1} R_o \cdot \mathsf{1}_{A_i=a}}{\sum_{i=1}^{t-1} R_o \cdot \mathsf{1}_{A_i=a}}$     

- 조건서술어는 ‘조건 서술어’가 참이면 1 아니면 0을 가지는 확률 변수를 나타낸다.
- 분모가 0이 되는 경우에는 *Qt*(*a*)도 0으로 정의한다.

분모가 무한으로 커지는 경우에는 큰 모집단에서 무작위로 뽑은 표본의 평균이 전체 모집단의 평균과 가까울 가능성이 높다는 기본 개념인 큰수의 법칙에 따라 *Qt*(*a*)는 *q*∗(*a*)로 접근한다.

*두번째, 행동의 선택*

### Greedy 방법

이렇게 추정한 가치로 행동을 선택하는 가장 간단한 규칙으로 그리디(Greedy) 행동 선택 방법이 있다. 가치가 최대인 행동 중 하나를 선택하는 것이다. 여러개가 있다면 무작위로 그중 하나를 선택할 수 있다. 아래와 같이 표현할 수 있다.

     $A_t \doteq \underset{a}{arg \ max} \ Q_t(a)$

이 전략은 내가 알고 있는 지식만을 활용(Exploitation)하기 때문에 탐험(Exploration)이 충분히 이루어지지 않았다는 점에서 한계가 있다. 탐욕적 행동 선택 방법을 대체할 만한 단순한 대안은 대부분은 탐욕적 선택을 유지하고 가끔씩 모든 행동을 대상으로 무작위 선택을 하는 **입실론 그리디(*ϵ*−*greedy*) 방법**이 있다.

### ***ϵ*−*greedy* 방법**

상대적 빈도수 *ϵ*을 작은 값으로 유지하면서 탐욕적 선택 대신 행동의 추정 가치와는 무관하게 모든 행동을 균등한 확률로 선택하는 방법이다. 장점은 이후 단계 수가 무한으로 커지면 모든 행동을 선택하는 횟수도 무한이 되어 모든 *Qt*(*a*)가 *q*∗(*a*)로 수렴한다는 것이다.

# **10중 선택 테스트 ( 10-Armed TestBed )**

그리디 행동 가치 방법과 입실론 그리디 행동 가치 방법이 상대적으로 얼마나 효과적인지 평가하기 위해 테스트로 두 방법을 수치로 비교한다. 탐욕적 방법과 두 개의 입실론 탐욕적 방법 (*ϵ*=0.01,0.1*ϵ*=0.01,0.1)을 비교한다.

### **테스트 환경**

- 열 번의 선택을 하는 다중 선택 문제 2000개를 랜덤하게 생성한다.
- 각 행동의 가치 *q*∗(*a*)는 평균이 0이고 분산이 1인 정규 분포(gaussian) 분포에 따라 선택된다.
- 각 학습 방법 별 1000 스텝을 수행한다.
- 실행을 2000번 독립적으로 수행하면서 학습 알고리즘의 평균 결과를 측정할 수 있다.

음영으로 표시되는 분포는 아래 그림과 같다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/f8b81e0f-8d04-49c6-917d-047094ca30ce/Untitled.png)

### **Average reward**

탐욕적 방법은 시작 직후에는 다른 방법에 비해 조금 더 빠르게 향상되지만, 결국에는 성능이 떨어진다. 가장 좋을 것으로 기대되는 *q*∗(3) 보상은 1.55 인 것에 비해 탐욕적 방법의 단계당 보상은 1 정도이다.

### **Optimal action**

그리고 탐욕적 방법은 약 30% 정도의 문제에서 최적의 행동을 찾는 것으로 보인다. 입실론 그리디 방법은 탐험을 계속 했기 때문에 최적 행동을 찾을 확률이 계속해서 증가했다. 결국에 더 좋은 성능을 보였다.

*ϵ*=0.1인 경우 더 많이 탐험 했고 더 빨리 최적 행동을 찾았지만 최적 행동을 선택한 시간 단계는 91%에 미치지 못한다. 91%를 해석하자면 여기에서 1%는 10개의 행동 중 하나가 최적의 행동이기 때문이다.

*ϵ*=0.01의 경우, 최적 행동을 찾는 것은 더 느리지만 결국 *ϵ*=0.1에 비해 두 그래프 모두에 좋은 결과를 보일 것이다. *ϵ*을 시간에 따라 감소시켜 가면서 최상의 결과를 보이는 행동을 찾는 방법도 가능하다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/cf64e39a-49e3-40a9-8ced-b819206d8a0e/Untitled.png)

다중 선택 문제를 보상의 분포가 비정상적(non-stationary)이고, 행동 가치가 시간에 따라 변한다고 가정한다면, 비-탐욕적 행동이 탐욕적 행동에 비해 더 큰 가치를 가지지 않는지 확인해야하기 때문에 탐험이 중요 해진다.

# **2.4 Incremental Implementation**

컴퓨터의 메모리와 시간 단계별 계산량이 제한된 경우에 수치 계산 측면에서 효율적으로 계산하는 방법이 있다. 수식을 간단하게 하기 위해 행동 하나만 고려한다. 행동이 i번째 선택된 후에 받는 보상을 *Ri*라고 했을 때, 행동이 n-1번 선택된 이후에 이 행동의 가치 추정값 *Qn*은 아래의 식으로 표현할 수 있다.

$Q_n \doteq \frac{R_1 + R_2 + \cdot \cdot \cdot + R_{n-1}}{n-1}$

모든 보상을 기록하고 추정값이 필요할 때마다 식으로 계산하는 방식인데 보상이 커질수록 컴퓨터의 메모리와 계산 능력에 대한 수요가 증가한다. 보상이 하나 더 관측될 때마다 그 값을 저장할 메모리가 추가로 필요하고, 식의 분자에 있는 합계를 계산하기 위한 계산 능력이 추가로 필요하다. 하지만 이 과정이 사실은 일정한 계산 능력만을 이용하여 평균값을 갱신하는 점증적 공식으로 쉽게 만들 수 있다.

$Q_{n+1} = Q_n + \frac{1}{n}[ R_n - Q_n ]$

이 갱신 규칙은 아래와 같이 일반적으로 표현할 수 있다.

$새로운 \ 추정값  \  ← \ 이전 \ 추정값 \  + \ 시간 \  간격의 \ 크기 [ 목푯값 - \ 이전 \ 추정값]$

- [목푯값 - 이전 추정값]은 추정 오차(error)를 나타낸다.

오차는 ‘목푯값’으로 한단계 접근할 때마다 감소한다. 사실 시간 간격의 크기는 시간 단계마다 다른데 시간 간격의 크기를 *α*로 나타내거나 더 일반적으로는 *αt*(*a*)로 표현한다. 아래는 점증적으로 계산한 표본평균과 입실론 그리디 행동 선택을 이용한 완전한 다중 선택 알고리즘의 의사코드이다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/6100dc30-18c0-4e75-b867-a232642fe4ad/Untitled.png)

- *bandit*(*a*) : 어떤 행동에 대한 보상을 계산하는 함수

# **2.5 Tracking a Nonstationary Problem**

정상(stationary) 다중 선택 문제에서는 보상값의 확률 분포가 시간이 지나도 변하지 않기 때문에 평균값을 사용하는 것이 좋다. 하지만 실제에서는 비정상(non-stationary)하기 때문에 최근의 보상일 수록 더 큰 가중치를 줘야하고 오래된 보상일수록 더 낮은 가중치를 부여해야한다. 고정된 시간 간격을 이용해서 가중치를 부여하는 방법이 가장 많이 사용된다. 고정된 시간 간격을 이용해 보상의 가중치를 부여하는 방법으로 수정한 점증적 갱신 규칙은 아래와 같다.

$Q_{n+1} \doteq Q_n + \alpha[R_n - Q_n] = (1-\alpha)^nQ_1 +\sum\limits_{i=1}^n \alpha(1-\alpha)^{n-i}R_i$

- *α*∈(0,1] : 시간 간격의 크기 ( 고정 )
- *Q*1 : 초기 추정값
- $Q_{n+1}$: 이 초기 추정값 *Q*1과 과거 보상값들에 대한 가중치가 적용된 평균
- *Ri* : 보상

보상 *Ri*에 주어진 가중치 *α*(1−*α*)*n*−*i*는 보상이 관측되고 난 후 남아있는 보상의 갯수인 n-i에 따라 결정된다. 1−*α*는 1보다 작기 때문에 보상에 주어지는 가중치는 등장할 보상의 갯수가 증가함에 따라 감소된다. 기하급수적으로 감소하기 때문에 기하급수적 최신 가중 평균(exponential recency-weighted average)라고 부르기도 한다.

시간 간격의 크기를 시간 단계에 따라 변화 시키는 것이 편리할 때도 있는데 행동 a를 n번째 선택한 이후에 받은 보상을 처리하는 데 이용할 시간 간격의 크기를 *αn*(*a*)라고 하면, 표본 방법은 *αn*(*a*)=*n*1이 될 때이다. 표본 방법은 큰 수의 법칙에 따라 행동 가치의 참값으로 수렴함을 보장한다. 하지만 모든 선택 가능한 시간 간격 크기 *αn*(*a*)에 수렴성이 보장 되는 것은 아니다. 확률론적 근사 이론에 따르면 100% 확률로 수렴성을 보장하기 위한 조건은 아래와 같다.

1. $∑_{n=1}^{∞} α_n(a) = ∞$ : 어떤 초기 조건이나 확률적 변동성도 극복할 만큼 충분히 큰 시간 간격을 보장한다.
2. $∑_{n=1}^{∞} αn²(a) = ∞$ : 시간 간격이 감소해서 결국에는 수렴성을 확신할 만큼 충분히 작아진다.

표본 방법의 경우에는 두 가지를 모두 만족하지만 고정된 시간 간격의 크기 *αn*(*a*)=*α*를 적용하면 만족할 수 없다. 충분히 작아져야 한다는 두 번째 조건을 만족하지 못하는데, 이것은 추정값이 완전히 수렴하지 않고 가장 최근에 받은 보상에 반응하여 연속적으로 변한다는 것을 의미한다. 앞에서 언급한 것과 같이 실제 문제에서는 당연한 것이다. 그렇기 때문에 실제 문제에서는 적용이 거의 되지는 않는다.

## **2.6 Optimal Initial Value**

**탐험Exploration**을 촉진하는 기법을 긍정적 초깃값(Optimal initial value)라고 부른다.

이전에 언급된 방법들은 행동 가치의 초기 추정값 *Q*1(*a*)의 영향을 받았다. 초깃값에 편중 되어(Biased) 있었다. 편향 되었다는게 나쁜 것만은 아니다. 편향된 정보를 사전 지식으로 초기 추정값을 정할 수 있는 장점이 있다.

액션 들의 가치에 대한 초기 추정값이 긍정적이면, 학습자는 액션에 대한 보상값에 실망해서 다른 행동을 선택하게 된다. 결과적으로 가치 추정값이 수렴하기 전까지 탐험Exploration이 된다.

아래의 그림은 이전의 10-Armed Bandits에서 모든 행동 a에 대해

- *Q*1(*a*)=5를 사용한 그리디 메소드
- *Q*1(*a*)=0을 사용한 입실론 그리디 메소드

를 적용 했을 때 결과를 함께 보여준다. 실험 초기에는 탐험이 많은 그리디 방법이 안좋은 결과를 보이지만, 시간이 지남에 따라 탐험Exploration이 줄어들기 때문에 궁극적으로는 더 좋은 성능을 보여준다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/73c36cc7-8243-4d96-a80b-7ced428787ce/Untitled.png)

하지만 초기 조건에 맞춘다면 비정상적(non-stationary)문제에는 도움이 되지 않는다. 초깃값에 의존하는 것은 아래와 같은 문제가 있다.

- Exploration에 대한 원동력이 본질적으로 일시적이다.
- 새로운 Action에 대해서도 탐험Exploration이 필요하다.

그렇기 때문에 초기 조건에 너무 많이 초점을 맞추는 것은 좋지 않다는 결론을 얻을 수 있다.

## **2.7 Upper Confidence Bound Action Selection**

입실론 그리디 방법은 비 탐욕적 행동을 선택하는 것을 강제하지만, 행동을 선택할 때 탐욕적 행동과 아닌 것 중 어느 쪽도 선호하지 않고 선택한다.

- 행동 가치의 추정값이 최대치에 얼마나 가까운지?
- 추정의 불확실성이 얼마인지?

를 고려하는 것이 중요하다. 아래의 식은 효과적인 행동 선택 방법 한가지를 제안한다.

$A_t \doteq \underset{a}{arg \ max} \ [ Q_t(a) + c \sqrt{\frac{ln \ t}{N_t(a)}}]$

- *ln* *t* : 시각 t 에 대한 자연 로그
- *Nt*(*a*) : 시각 t 이전의 행동 a를 선택한 횟수
- *c*>0 : 불확실성의 정도 ( 탐험Exploration의 정도 )

행동 a의 가치에 대한 추정값의 불확실성 또는 편차를 함께 고려한다. 위 식의 최댓값은 행동 a의 가치로 사용가능한 값들의 상한값(Maximum Bound)이 되며, 파라미터 c를 사용해서 상한의 신뢰 수준을 결정한다. 특정 액션 a가 선택될 때마다 불확실성은 다음과 같이 변화한다.

- a 가 선택 될 때, 분모의 *Nt*(*a*)가 증가하면서 불확실성이 감소한다.
- a 가 선택 되지 않을 때, *ln* *t* 만 증가하면서 불확실성이 증가한다.

모든 행동이 선택 되지만 더 작은 가치 추정값을 갖는 행동이나 이미 자주 선택된 행동은 시간이 지남에 따라 선택되는 빈도수가 작아진다.

아래의 그림은 마찬가지로 10-Armed Bandit에서 UCB를 적용한 결과이다. 처음을 제외하면 일반적으로 UCB가 성능이 더 좋다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/0360ffcc-4eac-4939-bf27-8c60f0c0b19f/Untitled.png)

책의 뒷 부분에서 다루는 UCB를 더 일반적인 강화학습 문제로 확장하는 방법은 입실론 그리디 방법보다 어렵다. nonstationary 문제를 다루고 더 큰 상태 공간을 다루기 때문이다.

## **2.8 Gradient Bandit Algorithm**

각 행동 a에 대한 수치적 선호도(preference)를 학습하는 방법이다. 행동의 가치를 추정하는 방법이 아니라 선호도를 기반으로 행동을 선택하는 방법이다. 선호도(*Ht*(*a*))가 큰 행동이 더 자주 선택된다. 선호도는 보상과는 다른 개념이다.

각 행동을 선택할 확률은 소프트 맥스 분포*soft*−*max* *distribution*(*Gibbs* 또는 *Boltzman* 분포)에 따라 다음과 같이 결정된다.

$Pr {\{ A_t = a}\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} \doteq \pi_t(a)$

- *πt*(*a*) : 시각 t에 행동 a를 선택할 확률
- *H*1(*a*)=0 : 처음의 모든 행동 선호도는 같다.

확률론적 경사도 증가(stochastic gradient ascent)방법을 활용하는 학습 알고리즘으로 행동 *At*를 선택한 후에 보상 *Ri*를 받는 모든 단계에 대한 선호도를 아래와 같이 갱신한다.

  $H_{t+1}(A_t) \doteq H_t(A_t) + \alpha(R_t - \bar{R_t})(1-\pi_t(A_t))$

그리고

$H_{t+1}(a) \doteq H_t(a) - \alpha(R_t - \bar{R_t})\pi_t(a)$                   

모든 $a \neq A_t$에 대해

- *α*>0 : 시간 간격의 크기
- $\bar{R}_t \in R$ : 시간 t 까지의 모든 보상에 대한 평균

현재의 보상과 비교하는 대상으로 $\bar{R}_t$를 사용한다.

- $R_t>\bar{R}_t$ 인 경우, 미래에 *At*를 선택할 확률 *πt*+1(*a*)이 증가한다.
- $R_t<\bar{R}_t$인 경우, 미래에 *At*를 선택할 확률 *πt*+1(*a*)이 감소한다.

선택된 행동에 대한 확률은 증가하고, 선택 되지 않는 행동에 대한 확률은 반대로 변화한다.

아래의 그림은 마찬가지로 10-Armed Bandit에서 Gradient Bandit 알고리즘을 적용한 결과이다. 평균 4, 분산 1인 정규분포로 보상의 참값이 선택된다.

- 비교 대상이 있는 경우 ( with baseline )
- 비교 대상이 없는 경우 ( without baseline )

에 대해서 결과를 보이는데 비교 대상이 없는 경우에는 성능이 상당히 저하된다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/38c7b8d6-1791-4993-b00a-32874dc22816/Untitled.png)

경사도 증가(gradient ascent) 규칙을 따르면, 행동의 선호도 *Ht*(*a*)의 증가량은 선호도 증가가 성능에 미치는 효과와 비례한다.

$H_{t+1}(a) \doteq H_t(a) + \alpha\frac{\partial E[R_t]}{\partial H_t(a)}$

여기서 *E*[*Rt*]인 성능 측정 값은 보상의 기댓값이 된다.

$E[R_t] = \underset{x}\sum\pi_t(x)q_*(x)$

선호도 증가가 성능에 미치는 효과의 측정은 이 성능 지표를 행동의 선호도를 편미분*partial* *derivative*한 것을 통해 이루어진다. 성능의 경사돗값(gradient)에 대해 살펴보면

$\frac{\partial E[R_t]}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)}[\underset{x}\sum\pi_t(x)q_*(x)] = \underset{x}\sum{q_*(x)\frac{\partial \pi_t(x)}{\partial H_t(a)}} = \underset{x}\sum{(q_*(x) - B_t)\frac{\partial\pi_t(x)}{\partial H_t(a)}}$

- *Bt* : 비교 대상 ( Baseline )

비교 대상을 포함해도 등식에 변화가 없는게, 모든 행동에 대한 경사돗값의 총합이 0이다.

$\sum_x \frac{\partial \pi_t(x)}{\partial H_t(a)} = 0$

*Ht*(*a*)가 변함에 따라 어떤 행동을 선택할 확률은 증가하고 다른 행동을 선택할 확률은 감소하겠지만, 확률의 합은 1이기 때문에 행동을 선택할 확률의 증가와 감소의 총합은 항상 0이 된다. 따라서

$= E [(q_*(A_t) - B_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)} /\pi_t(A_t)]$

$= E [(R_t - \bar{R_t})\frac{\partial \pi_t(A_t)}{\partial H_t(a)} /\pi_t(A_t)]$

바로$∂πt(At)  / ∂Ht(a) = πt(x) (1_{a=x} - πt(a))$을 얻을 수 있다.

- a=*x* 일 경우, $1_{a=x}=1$ 이 된다.
- a≠x 일 경우, $1_{a=x}=0$ 이 된다.

$= E [(R_t - \bar{R_t})\pi_t(A_t)(1_{a=A_t} - \pi_t(a))/\pi_t(A_t)]$

$= E [(R_t - \bar{R_t})(1_{a=A_t} - \pi_t(a))]$

**계획 :** 경사돗값을 각 단계에서 추출한 어떤 표본의 기댓값(표본평균)으로 표현하는 것

성능의 경사돗값을 위의 표본평균으로 대체하면 다음과 같은 수식이 된다.

모든 a에 대해 $*H_{t+1}(a)=α(R_t-\bar{R}_t)(1_{a=A_t}−π_t(a))*$

$∂πt(At) / ∂Ht(a) = πt(x) (1_{a=x} - πt(a))$의 성립함을 보여야 한다.

몫의 미분법을 그대로 적용하면

$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)}\pi_t(x)$

$= \frac{\partial}{\partial H_t(a)}[\frac{e^{H_t(x)}}{\sum^k_{y=1} e^{H_t(y)}}]$

$= \frac{\frac{\partial e^{H_t(x)}}{\partial H_t(a)}\sum^k_{y=1}e^{H_t(y)}-e^{H_t(x)}\frac{\partial \sum_{y=1}^k e^{H_t(y)}}{\partial H_t(a)}}{(\sum^k_{y=1} e^{H_t(y)})^2}$

$= \frac{1_{a=x}e^{H_t(x)}\sum_{y=1}^k e^{H_t(y)} - e^{H_t(x)}e^{H_t(a)}}{(\sum_{y=1}^k e^{H_t(y)})^2}$

$= \frac{1_{a=x}e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}} - \frac{e^{H_t(x)}e^{H_t(a)}}{(\sum_{y=1}^k e^{H_t(y)})^2}$

$= 1_{a=x}\pi_t(x) - \pi_t(x)\pi_t(a)$

$= \pi_t(x)(1_{a=x} - \pi_t(a))$

위 과정으로 두가지를 확인할 수 있다.

- 경사도 다중 선택 알고리즘의 갱신의 기댓값 = 보상의 기댓값에 대한 경사돗값
- 알고리즘이 확률론적 경사도 증가의 한 종류이다.

확률론적 경사도 증가의 한 종류라는 것은 강건한 수렴성을 갖는다는 것을 보장한다. 비교대상값의 특성에 대해서 어떠한 전제 조건도 없다. 그래도 이 알고리즘은 여전히 확률론적 경사도 증가의 한 종류가 된다.

추가로 비교대상값(Baseline)을 어떤 것을 선택하느냐는 갱신되는 값의 기댓값에 영향을 주지 않지만 갱신되는 값의 분산에는 영향을 주고 분산으로 인해 기댓값의 수렴 속도가 영향을 받는다.

## **2.9 Associative Search (Contextual Bandits)**

일반적인 강화학습 문제에서는 고려해야할 여러 상황이 있고 강화학습의 목적은 정책을 학습하는 것이다.

정책 : 어떠한 상황으로부터 그 상황에 맞는 최고의 행동을 도출하는 대응 관계

### **연관 탐색 (associative search)**

시행착오 학습으로 최고의 행동을 탐색하는 동시에 이 행동이 최고가 되는 상황을 이 행동과 연관시킨다. 이것을 맥락적 다중 선택(contextual bandits)이라고도 한다. Contextual Bandit은 완전한 강화 학습이라고도 할 수 없고 완전한 다중 선택 문제라고도 할 수 없지만 아래와 같은 유사성은 있다.

- 정책에 대한 학습을 포함한다. ( 강화 학습과 같음 )
- 행동이 그 순간의 보상에만 영향을 준다 ( 다중 선택 문제와 같음 )

# **2.10 요약**

활용Exploration과 탐험Exploitaion의 Trade-Off를 해결하기 위한 여러가지 간단한 방법을 제시했다.

- 입실론 그리디 : 좁은 시간 구역을 무작위로 선택
- UCB : 행동을 선택할 때 결정론적 방법을 취하지만, 각 단계에서 그때까지 얻은 표본의 수가 적은 행동을 미묘하게 선택한다.
- 경사도 다중 선택 : 행동의 선호도를 추정하고 소프트 맥스 분포를 활용해서 등급에 따른 확률적 방식으로 선호되는 행동을 선택한다.
- 초기 추정값 : 초깃값을 긍정적으로 설정하는 방법으로 그리디 한 방법에 Exploration을 추가한다.

위의 방법 중 가장 좋은 알고리즘을 선택하는 것은 문제가 있는데 모두 파라미터를 가지고 있기 때문이다. 파라미터에 따라 성능이 달라지므로 각 시간 단계에 대한 평균값으로 완전한 학습 곡선(learning curve)를 그린다. U자를 뒤집어 놓은 모양으로 최적의 파라미터는 중간 정도에 위치한다는 것으로 판단할 수 있다. 물론 파라미터의 변화에 따라 얼마나 성능이 변화하는지도 평가해야한다. 다중 선택 문제에서는 전반적으로 UCB가 가장 좋은 성능을 보였다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8048443c-953b-486d-a3ce-04e1fc441172/b6f73136-ea49-43e3-ab5e-1e769e26c8e6/Untitled.png)

### **기틴스 인덱스 ( Gittins index )**

다중 선택 문제에서 Exploration과 Exploitation 딜레마를 해결하는 방법은 특별한 행동 가치를 계산하는 것이다.

기틴스 인덱스는 행동 가치의 초기 분포에 대한 지식에 기반해서 각 단계 이후에 분포를 정확히 갱신하는 베이지안 방법의 일종인데 베이지안은 정보 업데이트를 위한 계산이 복잡할 수 있지만, 켤레 사전 분포(Conjugate Prior Distribution)라는 특별한 분포를 사용해서 쉽게 계산할 수 있다.

계산을 한 후, 각 단계에서 어떤 행동이 최선의 행동이 될 것인지에 대한 사후 확률(posterior probability)에 따라 행동을 선택한다. 사후 표본추출(posterior sampling)또는 톰슨 표본추출(Thompson sampling)이라고 불리는 이 방법은 분포와 관계없는(distribution-free) 방법이 보여주는 가장 좋은 성능과 대체로 유사한 성능을 보여준다.

베이지안 환경으로 Exploration과 Exploration 사이의 최적 균형을 계산하는 것도 가능하다. 모든 가능한 행동에 대해 즉각적으로 주어질 수 있는 보상의 확률과 그에 따른 행동 가치에 대한 사후 분포를 계산할 수 있다. 따라서 그 중 가장 좋은 것을 선택하기만 하면 된다. 하지만 두개의 행동과 두개의 보상이 있더라도 총 1000번의 타임 스탬프인 경우 계산량이 2200022000으로 실현 불가능한 양이다. 하지만 근삿값을 효율적으로 계산하는 것은 가능하다. 이러한 접근법으로 다중 선택 문제가 완전한 강화 학습의 한 종류가 되게끔 효과적으로 변화시킬 수 있다.
