본 시리즈는 리처드 서튼, 앤드류 비르토 공저 "단단한 강화학습(Reinforced Learning An Introduction, 2nd edition)"을 통한 강화학습 스터디 목적으로 그 내용을 요약, 정리하여 기록한 시리즈입니다.

---
### 개요
1. 동적 프로그래밍(Dynamic Programming, DP)
   - 동적 프로그래밍 : 마르코프 결정 과정(MDP)과 같은 환경 모델이 완벽히 주어졌을 때 최적 정책을 계산하기 위해 사용될 수 있는 일군의 알고리즘
   - 고전적인 DP 알고리즘은 완벽한 모델과 엄청난 양의 계산을 필요로 하여 강화학습에서의 활용도가 제한됨
   - 그러나 DP 알고리즘은 이론적으로는 현재까지도 매우 중요한 개념
   
2. 기본적 전제
   - 환경은 유한 MDP로 모델링된다.
      - 환경의 상태 $S$, 행동 $A$, 보상 $R$의 집합이 유한 집합이다.
      - 환경의 동역학이 모든 $s \in S$, $a \in A(s)$, $r \in R$, $s' \in S^{+}$에 대한 확률 $p(s', r|s, a)$로 주어진다.<br>
      ※ 에피소딕 문제의 경우 $S^+$는 $S$와 종단 상태의 합이다.
   - 앞서 [마르코프 결정 과정](https://velog.io/@mino0121/Reinforced-Learning-Finite-Markov-Decision-Process)에 대해 다룰 때 확인했듯, 최적 가치 함수 $v_{\*}$와 $q_{\*}$는 최적 벨만 방정식을 만족한다.
      - $v_{\*}(s)=\displaystyle{\max_{a}\mathbb{E}[R_{t+1}+\gamma v_{\*}(S_{t+1})|S_{t}=s, A_{t}=a]}$<br>
      = $\displaystyle{\max_{a}\sum_{s', r}p(s', r|s, a)[r+\gamma v_{\*}(s')]}$
      - $\displaystyle{q_{\*}(s, a)=\mathbb{E}[R_{t+1}+\gamma \max_{a'}q_{\*}(S_{t+1}, a')|S_{t}=s, A_{t}=a]}$<br>
      = $\displaystyle{\sum_{s', r}p(s', r|s, a)[r+\gamma \max_{a'}q_{\*}(s', a')]}$
   - 상기와 같은 벨만 방정식을 목표 가치 함수에 대한 근사를 향상시키기 위한 할당의 형식(갱신 규칙)으로 변환함으로써 DP 알고리즘을 얻을 수 있다.
   
---
### 정책 평가(예측)
1. 정책 평가 개요
   - 임의의 정책 $\pi$에 대해 상태 가치 함수 $v_\pi$를 계산하는 방법
   > $\displaystyle{v_{\pi}\doteq\mathbb{E}\_{\pi}[G_{t}|S_{t}=s]}$<br>
   = $\displaystyle{\mathbb{E}\_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s]}$<br>
   = $\displaystyle{\mathbb{E}\_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s]}$<br>
   = $\displaystyle{\sum_{a}\pi(a|s)\sum_{s', r}p(s', r|s, a)[r+\gamma v_{\pi}(s')]}$
      - $\pi(a|s)$ : 정책 $\pi$하에서 상태 $s$에 있을 때 행동 $a$를 선택할 확률
      - 기댓값을 나타내는 기호의 아래첨자 $\pi$ : 정책 $\pi$를 따른다는 것을 전제로 한 조건부 기댓값임을 의미
   - $\gamma<1$을 만족하거나 정책 $\pi$를 따르는 모든 상태가 종국적으로 더 이상 변하지 않는 상태에 도달한다는 것이 담보될 경우 $v_{\pi}$의 존재와 유일성이 보장됨
   → 말 그대로 정책 평가를 반복했을 때 $\gamma$의 값이 더 이상 변하지 않으면 그것이 최적 가치 함수가 된다는 의미
2. 반복 정책 평가(Iterative policy evaluation)
   - 환경의 동역학을 완전히 알고 있을 경우 다음의 식은 집합 $|S|$의 원소 개수만큼의 미지수($v_{\pi}(s), s\in S)$를 갖는 선형 연립 방정식이 되며, 방정식의 개수  역시 집합 $|S|$의 원소 개수와 동일함
   >  $v_{\pi}(s)\doteq\displaystyle{\sum_{a}\pi(a|s)\sum_{s', r}p(s', r|s, a)[r+\gamma v_{\pi}(s')]}$
   - 가치 함수의 초기 근삿값 $v_0$을 임의로 선택할 경우 이어지는 근삿값은 상기 식을 위한 벨만 방정식을 갱신 규칙으로 하여 모든 $s \in S$에 대해 다음과 같이 구해진다.
   > $v_{k+1}(s)\doteq\displaystyle{\mathbb{E}\_{\pi}[R_{t+1}+\gamma v_{k}(S_{t+1})|S_{t}=s]}$<br>
   = $\displaystyle{\sum_{a}\pi(a|s)\sum_{s', r}p(s', r|s, a)[r+\gamma v_{k}(s')]}$
   - $v_{k}=v_{\pi}$일 경우 벨만 방정식의 등호가 성립하므로 상기 갱신 규칙을 따른다면 $v_{k}=v_{\pi}$의 일정한 값을 가짐
   - 즉, '반복 정책 평가'란 $v_{\pi}$의 존재를 보증하는 조건하에서 $k$ → $\infty$일 때 일반적으로 수열 $\{v_{k}\}$가 $v_{\pi}$로 수렴하는 알고리즘을 말한다.
3. 기댓값 갱신(Expected update)
   - 근삿값 $v_k$로부터 $v_{k+1}$을 연속적으로 구하기 위해 모든 상태 $s$에게 동일하게 적용하는 작동 방식
   - 현재 평가받고 있는 정책하에서 일어날 수 있는 모든 단일 단계 전이(One-step transition)에 대해 $s$의 이전 가치를 새로운 가치로 대체하는 것
   - 이때 새로운 가치는 $s$ 이후에 나타나는 상태들의 이전 가치와 즉각적인 보상의 기댓값을 이용하여 산출함
   - DP 알고리즘에서 수행되는 모든 갱신은 '기댓값 갱신'으로 지칭한다.
4. 반복 정책 평가를 위한 배열
   - 상기 반복 정책 평가를 구현하기 위한 프로그램을 작성하고자 할 경우 두 개의 배열을 사용
      - 이전 가치( $v_{k}(s)$ )를 위한 배열
      - 새로운 가치( $v_{k+1}(S)$ )를 위한 배열
   - 그러나 새로운 가치가 이전 가치를 덮어씌우도록 하는 방식으로 하나의 배열만을 사용하는 것도 가능하며, 이 알고리즘 역시 $v_{\pi}$로 수렴함
   - 오히려 하나의 배열을 사용하는 경우가 두 개의 배열을 사용하는 경우보다 더 빠르게 수렴한다(새로운 데이터가 생기자마자 바로 이용할 수 있으므로).
   
---
### 정책 향상(Policy improvement)
1. 정책 향상 정리(Policy improvement theorem)
   - 기본 전제
      - 어떤 정책에 대해 가치 함수를 계산하는 이유는 '더 좋은 정책'을 찾기 위한 것
      - 임의의 결정론적 정책 $\pi$에 대하여 가치 함수 $v_{\pi}$를 셜정했다고 가정했을 때, 상태 $s$에서 현재 정책($\pi$)을 따르는 것에 대하여
         - 이것이 얼마나 좋은지는 $v_{\pi}(s)$를 통해 계산 가능
         - '정책 자체를 새로운 것으로 바꾸는 것'이 더 좋은지 더 나쁜지에 대해서는 어떻게 답할 것인가?
      - 상기 질문에 댑하는 방법 중 하나는 상태 $s$에서 행동 $a$를 선택하고, 그 후에는 현재의 정책 $\pi$를 따를 때의 값이 $v_\pi(s)$보다 큰지 작은지를 판단하는 것
      > $q_\pi(s, a)\doteq\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}=a]$<br>
      = $\displaystyle{\sum_{s', r}p(s', r|s, a)[r+\gamma v_{\pi}(s')]}$
   - 개요
      - 임의의 결정론적 정책 $\pi$와 $\pi'$이 모든 $s\in S$에 대해 다음의 부등식을 만족한다.
        > $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$
      - 이는 다시, 정책 $\pi'$이 모든 상태 $s\in S$로부터 도출하는 이득의 기댓값이 정책 $\pi$가 도출하는 것보다 크거나 같음을 의미한다.
        > $v_{\pi'}(s) \geq v_{\pi}(s)$
      - 이때 변경된 정책 $\pi'$은 $\pi'(s)=a\neq\pi(s)$라는 차이점을 제외하면 원래의 정책 $\pi$와 동일하다.
      - 따라서 $q_{\pi}(s, a)>v_{\pi}(s)$를 만족할 경우, 변경된 정책($\pi'$)은 정책 $\pi$보다 더 좋은 결과를 가져온다.
2. 정책 향상(Policy improvement)
   - 상기 '정책 향상 정리'는 정책과 그것의 가치 함수가 주어졌을 때 '하나의 상태'에서 어떤 행동을 선택하는 정책의 변화를 어떻게 평가할 것인지에 대한 내용
   - '정책 향상 정리'를 확장하여 '모든 상태'와 선택 가능한 '모든 행동'에 대하여 $q_{\pi}(s, a)$의 측면에서 가장 좋아 보이는 행동을 선택하게 하는 '정책의 변화'를 정책 향상이라 한다.
   - 다시 말해 정책 향상은 기존 정책의 가치 함수에 대하여 '탐욕적'이 되게 함으로써 기존 정책을 능가하는 새로운 정책을 만드는 과정을 말한다.
     > $\pi'(s)\doteq\displaystyle{\text{argmax}\_{a}q_{\pi}(s, a)}$<br>
   = $\displaystyle{\text{argmax}\_{a}\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}=a]}$<br>
   = $\displaystyle{\text{argmax}\_{a}\sum_{s', r}p(s', r|s, a)[r+\gamma v_{\pi}(s')]}$
   - 만약 새로운 탐욕적 정책 $\pi'$이 기존 정책 $\pi$를 능가하지 못하고 기존 정책과 동일한 수준을 유지한다면 $v_{\pi}=v_{\pi'}$이 되고, 모든 $s\in S$에 대하여 다음 관계가 성립한다.
     > $v_{\pi'}(s) = \displaystyle{\max_{a}\mathbb{E}[R_{t+1}+\gamma v_{\pi'}(S_{t+1})|S_{t}=s, A_{t}=a]}$<br>
   = $\displaystyle{\max_{a}\sum_{s', r}p(s' ,r|s, a)[r+\gamma v_{\pi'}(s')]}$
   
      - 이 식은 최적 벨만 방정식과 동일하며, 따라서 $v_{\pi'}$은 $v_{*}$가 되고, $\pi$와 $\pi'$ 모두 최적 정책이 된다.
      - 따라서 기존 정책이 이미 최적 정책이 아닌 경우, 정책 향상은 반드시 더 향상된 정책을 도출해야 한다.
3. 정책 향상 단계에서의 행동 선택
   - 상기 정책 향상 단계에서 선택 가능한 행동이 유일하지 않은 경우(즉, 최대로 만드는 행동이 여러 개인 경우) 여러 개의 행동 중 하나의 행동만을 선택할 필요가 없다.
   - 탐욕적 정책에서 '최대화 행동'을 제외한 나머지 행동에 대하여 0의 확률을 부여하기만 한다면, 여러 개의 최대화 행동에 대해 확률을 배분하는 방법은 어떤 것이든 상관 없다.

---
### 정책 반복(Policy Iteration)
- 정책 평가와 정책 향상의 반복을 통해 최적의 정책을 찾아내는 알고리즘
  > - 정책 $\pi_{0}$에 대하여 상태 가치 함수 $v_{\pi_{0}}$를 계산한다.<br>
    - 가치 함수 $v_{\pi_{0}}$를 이용해 정책 $\pi_{0}$를 더 좋은 정책  $\pi_{1}$으로 향상시킨다.<br>
    - 정책 $\pi_{1}$에 대하여 상태 가치 함수 $v_{\pi_{1}}$을 계산한다.<br>
    - 가치 함수 $v_{\pi_{1}}$을 이용해 정책 $\pi_{1}$을 더 좋은 정책  $\pi_{2}$로 향상시킨다.<br>
    ...<br>
    - 정책 $\pi_{n}$에 대하여 상태 가치 함수 $v_{\pi_{n}}$을 계산한다.<br>
    - 가치 함수 $v_{\pi_{n}}$을 이용해 정책 $\pi_{n}$을 더 좋은 정책  $\pi_{n+1}$로 향상시킨다.
- 이전 정책이 '최적 정책'이 아닐 경우, 모든 정책은 반드시 바로 이전의 정책보다 더 나은 결과를 도출한다.
- 이때, 유한 MDP가 갖는 정책의 개수는 유한하므로, 상기의 과정을 '유한하게' 반복함으로써 최적 정책 및 최적 가치 함수로 수렴하는 것이 가능하다.<br>
※ 이때, 한 정책에서 다음 정책으로 넘어갈 때 이전 정책의 가치 함수로부터 계산을 시작하므로 정책 평가의 수렴 속도는 상당히 증가하게 된다.
- 정책 반복의 의사 코드(Psudo code)를 정리하면 다음과 같다.
> 1. 초기화<br>
  모든 $s \in S$에 대해 임의로 $V(s) \in \mathbb{R}$와 $\pi(s)\in A(s)$를 설정<br>
  2. 정책 평가<br>
  루프:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\displaystyle{\Delta}$ ← 0<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;모든 $s \in S$에 대하여 루프:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v$ ← $V(s)$<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s)$ ← $\sum_{s', r}p(s,' r|s, \pi(s))[r+\gamma V(s')]$<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\Delta$ ← $\max(\Delta, |v-V(s)|)$<br>
$\Delta<\theta$를 만족할 때까지(추정의 정밀도를 결정하는 작은 양수)<br>
  3. 정책 향상<br>
  안정적 정책 ← $true$<br>
  모든 $s \in S$에 대해:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;이전 행동 ← $\pi(s)$<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\pi(s)$ ← $\text{argmax}\_{a}\sum_{s', r}p(s', r|s, a)[r+\gamma V(s')]$<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;이전 행동 $\neq$ $\pi(s)$이면, 안정적 정책 ← $false$<br>
  안정적 정책이 $true$이면 멈추고, $V\approx v_{\*}$와 $\pi \approx \pi_{*}$를 반환하라. 그렇지 않으면 2번 과정으로 돌아가라.
  
---
### 가치 반복(Value iteration)
1. 정책 반복의 단점
   - 정책 반복은 정책 반복 주기마다 정책 평가를 수행한다는 단점이 있음
      - 정책 평가는 그 자체로 시간이 오래 걸리는 반복적 계산
      - 상태 집합에 대한 여러 번의 일괄 계산 수행이 필요
   - 정책 평가의 $v_{\pi}$로의 수렴은 극한의 측면에서만 가능
2. 가치 반복
   - 정책 반복의 과정에서 정책 평가 단계의 반복 계산 과정을 중간에 멈출 수 있는 방법 중 하나<br>
   ※ 계산 과정이 중간에 중단되더라도 정책 반복의 수렴성은 보장됨
   - 정책 평가의 계산 과정이 단 한 번의 일괄 계산(각 상태를 한 번 갱신하는 것) 이후에 중단되는 것을 '가치 반복'이라 한다.
   - 정책 향상과 중단된 정책 평가 단계를 결합하는 간단한 갱신 과정으로서 다음과 같이 표현 가능
     > $v_{k+1}(s)\doteq\displaystyle{\max_{a}\mathbb{E}[R_{t+1}+\gamma v_{k}(S_{t+1})|S_{t}=s, A_{t}=a]}$<br>
   = $\displaystyle{\max_{a}\sum_{s', r}p(s', r|s, a)[r+\gamma v_{k}(s')]}$
     - 상기 식은 모든 $s \in S$에 대하여 성립한다.
     - 임의의 $v_{0}$에 대해 $v_{\*}$의 존재를 보장하는 조건과 동일한 조건하에서 수열 $\{v_{k}\}$가 $v_{\*}$로 수렴한다.
   - 다시 말해 선택 가능한 모든 행동에 대한 가치함숫값에 대해 평가를 실시하는 것이 아니라, 최초의 정책 평가 결과로 나온 최대가치행동에 대해서만 반복을 진행하는 것이다.
   - 최적 벨만 방정식을 통해서도 가치 반복의 이해가 가능
     - 최적 벨만 방정식을 갱신 규칙으로 바꾸면 가치 반복을 위한 식이 된다.
   - 가치 반복 갱신은 정책 반복 갱신과도 동일
     - '모든 행동에 대해 최댓값을 취하는 것'만 제외하면 두 식은 거의 완전히 동일하다ㅏ.
3. 가치 반복의 종료
   - 이론적으로는 가치 반복이 정확히 $v_{*}$로 수렴하기 위해서는 무한 번의 반복이 필요
   - 그러나 실제 적용에서는 가치 함수의 변화가 아주 작은 값 이내로 들어올 경우 반복을 중단함
   - 정책 향상의 일관 계산 사이사이에 정책 평가의 일괄 계산을 여러 번 삽입함으로써 수렴 속도를 증가시킬 수 있음
   
---
### 비동기 동적 프로그래밍
1. 동적 프로그래밍(DP)의 주요 단점
   - DP는 MDP의 전체 상태에 대한 계산 과정을 포함하며, 이는 상태 집합에 대한 일괄 계산이 필요함을 의미함
   - 상태 집합의 크기가 매우 클 경우 한 번의 일괄 계산도 불가능할 정도로 계산량이 많을 수 있음
2. 비동기(Asynchronous) 동적 프로그래밍 개요
   - 상태 집합에 대해 체계적인 일괄 계산을 수행하지 않는, 개별적인(In-place) 반복 DP 알고리즘
   - 상태의 가치를 갱신하는 순서와 무관하게, 다른 상태의 가치를 이용할 수 있는 상황이라면 그 값이 무엇이든 그것을 이용해 해당 상태의 가치를 갱신함
   - 정확한 수렴을 위해서는 모든 상태의 가치 갱신이 필요하며, 따라서 갱신 과정이 어느 정도 진행된 후에는 모든 상태에 대해 갱신을 수행함
   - 갱신할 상태를 선택하는 데 있어 대단히 유연한 알고리즘
3. 비동기 DP 알고리즘의 활용
   - 알고리즘의 수렴 속도 향상을 목표로 갱신의 적용 대상 상태를 선택
   - 하나의 상태에서 다른 상태로의 효율적인 가치 정보 전파를 위하여 갱신 순서를 조정
   - 최적 행동과 관련 없는 상태의 갱신을 완전히 건너뛰는 것도 가능
   - 에이전트가 실제로 MDP를 경험하면서 동시에 반복적 DP 알고리즘을 실행하도록 함
   - 초점 맞추기
      - 에이전트가 어떤 상태를 마주할 때 그 상태에 갱신을 적용하는 것
      - 상태 집합의 원소들 중 에이전트와 가장 관련이 있는 상태에만 '초점을 맞추어' DP 알고리즘의 갱신을 적용
---
### 일반화된 정책 반복(Generalized Policy Iteration, GPI)
1. 정책 반복의 구성
   - 정책 반복은 동시에 서로 상호작용하는 두 개의 과정으로 구성
      - 정책 평가 : 가치 함수가 현재의 정책을 잘 따르도록 하는 것
      - 정책 향상 : 정책을 현재 가치 함수에 대한 탐욕적 정책으로 만드는 것
   - 정책 반복을 구성하는 두 과정은 서로 번갈아 나타남
   - 두 과정의 반복으로 모든 상태가 갱신되면 그 최종 결과는 일반적으로 동일하다. 즉, 최종적으로 최적 가치 함수와 최적 정책으로 수렴한다.
2. 일반화된 정책 반복
   - 정책 평가와 정책 향상이 상호작용하는 일반적인 방법을 지칭하는 용어
   ※ 이때 이 상호작용은 정책 평가와 정책 향상의 반복 주기 및 세부 사항과는 무관함
   - 모든 강화학습 방법은 식별 가능한 정책과 가치 함수를 포함함
      - 정책은 가치 함수의 측면에서 향상됨
      - 가치 함수는 그 정책에 대한 가치 함수로 귀결됨
      - 평가와 향상 과정이 안정화되면(더 이상 변화를 만들지 않으면) 가치 함수와 정책이 최적화된 것
         - 가치 함수는 현재 정책을 따를 때만 안정화됨
         - 정책은 현재 가치 함수에 대해 탐욕적일 때만 안정화됨
         - 즉, 자기 자신의 가치 함수에 대해 탐욕적인 정책을 찾을 수 있을 때만 두 과정이 모두 안정화되며, 이는 정책과 가치 함수가 최적 벨만 방정식을 만족했음을 의미함
3. GPI에서의 평가와 향상
   - GPI에서 평가와 향상의 과정은 서로 경쟁하기도 하고 협력하기도 하는 관계
      - 두 요소는 서로를 반대 방향으로 당긴다는 면에서 경쟁적
         - 정책이 가치 함수에 대해 탐욕적이 되면 가치 함수는 변경된 정책에 대해 부정확해짐
         - 가치 함수가 정책을 따르도록 만들 경우 정책은 더 이상 탐욕적이지 않게 됨
      - 장기적 측면에서 두 과정이 서로 상호작용하여 하나의 공통 해(최적 가치 함수와 최적 정책)를 찾게 됨
      <img src="https://velog.velcdn.com/images/mino0121/post/e3a5d0ba-9e72-44aa-b492-4ba4d74ecdc3/image.png">
      
---
### 동적 프로그래밍의 효율성
1. DP 방법의 효율성
   - 기술적인 세부 사항을 제외하면, DP 방법이 최적 정책을 찾는 데 걸리는 시간은 상태와 행동의 개수에 대한 다항 함수로 나타남
   - 즉 상태의 개수(n)와 행동의 개수(k)에 대하여 DP 방법이 수행하는 계산 과정의 횟수는 n과 k에 대한 어떤 다항 함수의 값보다 작음
   - 따라서 DP는 정책 공간을 직접 탐색하는 다른 어떤 방법보다 더 빠르게(기하급수적으로) 최적 정책을 찾아냄
2. 선형 프로그래밍 방법과의 비교
   - 선형 프로그래밍 방법은 DP보다 최악의 조건에서 수렴성을 더 잘 보장함
   - 그러나 선형 프로그래밍 방법은 DP 방법이 다룰 수 있는 상태의 개수보다 훨씬 더 작은 상태에 대해서만 적용 가능
   - 규모가 가장 큰 문제에는 DP 방법만 적용 가능함
3. 큰 규모의 상태 공간을 다루는 문제
   - 큰 규모의 상태 공간을 다루는 문제에서는 비동기 DP 방식이 선호됨
   - 비동기 방법과 GPI의 변형을 적용함으로써 최적 정책을 동기화 방식보다 훨씬 더 빠르게 찾을 수 있음


