### 강화학습 개요
#### 강화학습
1. 강화학습이란
    - 기계학습에 해당하는 다른 어떠한 방법들보다 '상호작용'으로부터 배우는 목표 지향적인 학습에 더욱 초점을 맞춘 방법
    - '주어진 상황에서 어떤 `행동`을 취할지를 학습하는 것'
      - 행동의 결과는 최대한의 `보상(또는 이득)`을 가져다주어야 함
      - 보상 함수는 `수치적`으로 표현 가능해야 함
    - `시행착오`와 `지연된 보상`은 강화학습을 다른 학습 방법과 구분하는 중요한 두 가지 특성<br>
     ※ 지연된 보상 : 특정 행동이 그 행동에 직접적으로 영향을 받는 보상뿐만 아니라 그 다음에 이어지는 상황에도 영향을 미침으로써 연속적으로 보상에 영향을 미치는 것
    - 강화학습은 하나의 문제이기도 하고, 문제를 해결하는 방법이기도 하며, 문제와 해결 방법을 연구하는 분야이기도 함<br>
     ※ 동일하게 지칭하지만, 세 가지 개념을 명확히 구분할 수 있어야 한다(특히 문제와 그 해결 방법의 구별은 매우 중요).

2. 강화학습의 체계화
    - 강화학습은 불확실한 마르코프 결정 과정(Markov Decision Process, MDP)에 대한 최적 제어 이론을 활용하여 체계화됨
    - 기본적인 아이디어는 '목적을 위해 주변 환경과 상호작용하는 학습자가 직면하는 현실적인 문제의 가장 중요한 측면을 포착하는 것'
      - 학습자는 주변 환경의 상태를 어느 정도까지 `감지`하고, 그 상태에 영향을 주는 `행동`을 취할 수 있어야 한다.
      - 학습자는 주변 환경의 상태와 관련된 하나 이상의 `목표`를 가져야 한다.
    - 가장 간단한 형태의 마르코프 결정 과정은 '감지', '행동', '목표'라는 세 가지 측면만을 포함하는 것
    
3. 강화학습과 타 기계학습 분야와의 차이
    - 강화학습과 지도학습(Supervised learning)
      - 지도학습은 `지침(Label)`이 포함된 훈련 예제로부터 학습하는 방법으로서, '상호작용'으로부터 학습하는 것에는 적합하지 않다.
      - 상호작용으로부터 학습하는 문제에서는 `경험`에 의존하여 바람직한 행동(어떤 상황에 대한 적합한 대표성을 갖는 행동)의 예제를 찾는다.
      - 강화학습에서 학습자는 '미지의 영역'에서 자신의 경험으로부터 배울 수 있어야 하며, 이때 학습이 최대의 보상을 가져다준다.
    - 강화학습과 비지도학습(Unsupervised learning)
      - 비지도학습은 지침이 없는 데이터의 집합 내에서 `숨겨진 구조`를 찾는 방법이다.
      - 지침을 필요로 하지 않는다는 점에서 강화학습을 비지도학습의 한 종류로 생각할 수 있으나, 강화학습은 '숨겨진 구조'를 찾는 것을 목표로 하지 않는다.<br>
      ※ 숨겨진 구조를 찾는 것이 '보상을 최대로 만드는' 강화학습에 도움이 될 수는 있으나, 그것이 목적은 아니라는 점에서 큰 차이가 있다.

4. 강화학습의 '활용'과 '탐험'
    - 다른 학습에서는 찾기 어려운 강화학습만의 어려운 점은 '활용'과 '탐험' 사이의 절충
      - 활용(Exploitation) : 보상을 얻기 위해 과거에 보상의 획득에 효과적이었던 행동을 선호하는 것
      - 탐험(Exploration) : 더 큰 보상 획득에 효과적인 행동을 찾아내기 위해서 과거에 하지 않았던 행동을 시도하는 것
   - 활용과 탐험 둘 중 하나만 추구할 경우 강화학습의 목표를 이룰 수 없다.
   
5. 강화학습의 '학습자'
   - 강화학습은 '상호작용을 하는 완전하고 목표 지향적인 학습자'를 고려한 상태로 문제 해결을 시작한다.
   - 모든 강화학습 학습자는 분명한 목표가 있고, 주변 환경의 여러 측면을 감지할 수 있으며, 그 환경에 영향을 주기 위한 행동을 선택할 수 있다.
   - 또한 학습자는 자신이 마주한 환경이 불확실하더라도 학습을 수행해야 한다는 사실을 인지한 상태로 학습을 시작한다.
   - 문제의 각 하위문제들은 이 학습자 내부에서 분명한 역할을 해야 한다.
   
6. 강화학습의 상호작용
   - 강화학습은 공학 및 과학 분야와 실질적, 생산적인 상호작용을 한다.
   - 예를 들어 생태계의 학습 체계에서 영감을 받아 만들어진 강화학습의 핵심 알고리즘은 다시 동물학습의 심리학적 모델 혹은 뇌의 보상 시스템에 대한 믿을 만한 모델을 제공하였다.

7. 강화학습과 인공지능
   - 강화학습은 간단하면서도 일반적인 원리를 탐구하고자 하는 인공지능 연구의 경향성과도 부합한다.
   - 현대 인공지능 연구 상당수는 학습과 탐색, 의사결정의 일반 원리를 찾으려 하며, 강화학습은 이 과정에서 반드시 거쳐 가야 하는 지점이다.
    
#### 강화학습의 예
- 체스 선수가 말을 옮길 때 : 상대의 대응과 그에 대한 재대응을 예상하는 계획, 그리고 즉각적이고 직관적인 판단을 통해 말을 옮길 위치를 결정한다.
- 석유 정제 공장의 효율적 운영 : 적응 제어기를 이용해 엔지니어가 설정한 초기 파라미터를 조정한다.
- 로봇 청소기 : 현재 남아 있는 배터리의 양과 충전 스테이션의 위치 등을 고려한 과거 경험에 의존하여 더 많은 쓰레기를 모을지 충전을 위해 스테이션으로 돌아갈지를 결정한다.
- 인간의 아침 식사 : 복잡한 조건부 행동 및 목표와 하위 목표 사이의 상호 작용으로, 필요한 재료와 물건을 가져오는 일, 적절한 양의 재료를 투입하는 일 등 다양한 하위 문제들을 통해 '영양분의 섭취'라는 목표를 이루고자 한다.

상기와 같은 예시들은 '학습자와 그를 둘러싼 주변 환경 사이의 `상호작용`을 다룬다'는 특징을 공유한다. 이때 주변 환경에는 '불확실한' 요소들이 존재하며, 학습자는 '목표'를 이루기 위한 방법들을 모색한다.<br>
이때 주변 환경의 불확실성으로 인해 학습자는 행동의 결과를 완벽히 예측할 수 없으며, 따라서 학습자는 주변 환경을 지속적으로 모니터링하고 적절한 대응을 해야 한다. 그리고 그러한 행동 경험을 통해 자신의 행동 능력을 키우게 된다. 예컨대 그릇에 얼마큼의 우유를 부어야 적절한지를 직관적으로 깨닫게 되는 것이다.

---
### 강화학습의 구성 요소
'학습자'와 '주변 환경'을 제외하고도 강화학습에는 네 가지의 주요한 구성 요소가 존재한다.

1. 정책
   - 특정 시점에 학습자가 취하는 행동을 정의한다.
   - 심리학에서 말하는 자극-반응 규칙이나 그와 관련된 것들과 대응된다.
   - 정책 자체만으로도 행동을 결정할 수 있다는 점에서 정책은 강화학습 학습자에 있어 핵심이 되는 부분이다.
   - 정책은 확률론적으로 행동을 선택할 수도 있다.
   
2. 보상 신호
   - 강화학습이 성취해야 할 목표를 정의한다.
   - 주변 환경은 학습자에게 `보상`이라는 하나의 숫자, 즉 `보상 신호`를 전달하며, 학습자의 유일한 목표는 이 보상의 총합을 최대로 만드는 것이다.
   - 학습자는 보상 신호의 크기로부터 자신의 행동이 좋은 것인지 나쁜 것인지를 판단할 수 있다.
   - 보상 신호는 정책을 바꾸는 주된 요인으로 작용한다.
   
3. 가치 함수
   - 보상 신호가 '무엇이 좋은 것인가'를 '즉각적'으로 알려준다면, 가치 함수는 '장기적인 관점'에서 무엇이 좋은 것인가를 알려준다.
   - 특정 상태의 가치는 '그 상태의 시작점에서부터 일정 시간 동안 학습자가 기대할 수 있는 보상의 총량'을 말한다.
   - 즉, 가치는 특정 시점 이후의 상태와 그 상태에 포함된 장점을 고려하여 장기적 관점으로 평가한 상태의 장점이라고 할 수 있다.
   - 예컨대 '청소'를 하는 행위는 고통스러울 수 있지만(즉, 보상이 작을 수 있지만), '깨끗한 공간'이라는 상태에 포함된 장점이 크기 때문에 청소의 가치는 높다.
   - 행동을 선택할 때 보상이 최대인 행동보다는 가치가 최대인 행동을 선택해야 장기적으로 최대한 많은 보상을 얻을 수 있으므로, 결정을 내리고 그 결정을 평가할 때는 가치를 고려하게 된다.
   - 보상의 크기를 추정하는 것보다 가치의 크기를 추정하는 것이 더 어려우며(가치는 학습자의 생애주기 동안 반복적으로 추정되어야 하므로), 강화학습 알고리즘에서 가장 중요한 것은 효과적으로 가치를 추정하는 방법이다.
   
4. 환경 모델
   - 환경 모델은 환경의 변화를 모사하여, 환경이 어떻게 변화해 갈지를 추정할 수 있도록 한다.
   - 모델은 계획을 위해 사용되며, 이때 계획이란 미래의 상황을 실제로 경험하기 전에 가능성만을 고려하여 일련의 행동을 결정하는 방법을 의미한다.
   - 모델 기반(Model-based) 방법과 모델 없는(Model-free) 방법
      - 모델 기반 방법 : 모델과 계획을 사용하여 강화학습의 문제를 해결하는 방법
      - 모델 없는 방법 : 전적으로 시행착오만을 이용하여 학습자가 행동을 취하는 방법

---
### 상태와 진화적 방법
#### 상태
- 강화학습은 `상태(State)`라는 개념에 크게 의존한다.
- 상태
   - 정책과 가치 함수의 입력이 되기도 하고, 모델의 입력과 출력이 되기도 한다.
   - 특정 시각에 환경이 어떤 모습을 하고 있는지에 대한 정보를 학습자에게 전달하는 신호로 정의된다(비공식적).
   - 일반적으로는 '학습자가 사용할 수 있는 환경에 대한 모든 정보'라는 의미로 통용된다.

#### 진화적 방법
- 대부분의 강화학습 방법은 가치 함수를 추정하기 위한 것이나, 강화학습 문제를 풀기 위해 반드시 가치 함수를 추정해야 하는 것은 아니다.
- 유전자 알고리즘, 유전자 프로그래밍, 모의 담금질 등과 같은 최적화 방법은 가치 함수를 추정하지 않는다.
- 상기와 같이 오랜 시간 동안 불연속적 시간 간격으로 상호작용하는 다수의 정적 정책을 적용하는 방법을 `진화적(evolutionary)` 방법이라 한다.<br>
※ 진화적 방법은 별도로 다루지 않는다.

---
### 틱택토(tic-tac-toe)
#### 틱택토 게임
틱택토 게임은 세 개의 행과 세 개의 열로 이루어진 격자판에 2명이 서로 번갈아 O 혹은 X 표시(각 플레이어별로 정해진)를 하여 빙고와 같이 연달아 3개의 표시를 하는 쪽이 승리하는 게임이다. 어떻게 하면 상대방의 잘못된 선택을 찾아내고 승리 확률을 최대로 하는 법을 배우는 학습자를 만들 수 있을까?

#### 각 전통적 이론을 이용한 방법
1. 게임 이론의 미니맥스(minimax) 방법
   - 상대방이 특정한 방법으로 게임을 한다는 가정을 하므로 올바른 해결책이 될 수 없다.
2. 동적 프로그래밍
   - 어떤 상대방과 게임을 하더라도 최적의 해결책을 계산할 수는 있으나, 상대방에 대한 완벽한 정보(예를 들어 상대방이 특정 선택을 할 확률 등)가 필요하다.
   - 대부분의 실제 문제에서는 상대방에 대한 사전 정보가 제공되지 않으므로, 적절한 방법이 될 수 없다.
   - 상대방과 많이 게임을 해 봄으로써 상대방의 행동에 대한 학습을 하고, 이 근사적 모델과 동적 프로그램을 적용하여 해결책을 제시할 수 있다. → 이는 결과적으로 강화학습의 방법과 유사한 형태가 된다.
3. 진화적 방법
   - 가능한 정책들을 직접 탐색하여 승리 확률이 가장 높은 정책을 찾을 것이다.

#### 가치 함수를 이용한 접근법
1. 게임에서 나타날 수 있는 모든 상태에 숫자 하나씩을 부여하여 표를 생성한다.
   - 각 숫자는 해당 상태에서 게임 참여자가 승리할 확률에 대한 가장 최신의 추정값이다.
   - 이 추정값을 상태의 '가치'로 생각하면, 표는 학습된 가치 함수가 된다.
   - X를 표시하는 플레이어 기준으로, X가 연달아 3개 표시된 상태는 승리 확률이 1로, O가 연달아 3개 표시된 상태는 승리 확률이 0으로 설정된다.
2. 상대방과 여러 번 게임을 진행한다.
   - X를 표시할 위치를 선택하기 위해 자신의 선택이 불러올 상태를 면밀히 따져 보고, 현재 숫자 표의 가치를 확인하여 승리 확률 추정값을 최대로 하는 상태로 가도록 X를 표시할 위치를 '탐욕적'으로 선택한다.
   - 그러나 간혹 X를 표시할 위치를 무작위로 선택함으로써 새로운 경험을 얻기도 하는데, 이를 `탐험적 선택`이라 한다.
3. 게임을 하는 동안 게임 참여자는 자신이 처한 상태의 가치를 변화시킨다.
   - 자신이 처한 상태의 가치가 승리 확률에 대한 더 정확한 추정이 되도록 한다.
   - 탐욕스러운 선택 이후 결정될 상태의 가치를 선택 이전의 상태에 보강하여, 이전 상태의 현재 가치가 나중 상태의 가치에 가까워지도록 생신한다.
   - 이전 상태의 가치를 나중 상태의 가치와 가까워지는 방향으로 일정 부분 변경하는 방법으로, $V(S_{t}) ← V(S_{t}) + \alpha[V(S_{t+1})-V(S_{t})]$로 표시할 수 있다.
      - 탐욕스러운 선택 이전의 상태를 $S_{t}$, 선택 이후의 상태를 $S_{t+1}$이라 한다.
      - $\alpha$는 `시간 간격 파라미터`라고 불리는 작은 양의 값으로서, 학습 속도에 영향을 준다.
      - 이러한 갱신 규칙은 `시간차 학습 방법`의 일종에 해당한다.
      
#### 상기의 예가 설명하는 강화학습 방법의 핵심 특성
1. 강화학습은 주변 환경과 상호작용하며 학습하는 것을 강조한다.
2. 강화학습에는 확실한 목표가 있고, 올바른 행동을 위해 학습자가 선택한 행동의 지연된 효과를 고려하는 계획 또는 예지가 필요하다.

#### 상기의 예와 비교한 강화학습의 특징
1. 틱택토 게임은 두 사람이 하는 게임이지만, 강화학습은 보다 복잡한 문제, 예컨대 외부로부터의 작용이 없는 경우에도 적용할 수 있다.
2. 또한 틱택토 게임과 같이 학습자의 행동이 여러 개의 에피소드로 분리되어 있고 마지막 에피소드에만 보상이 주어지는 문제뿐만 아니라, 무한히 계속되고 다양한 크기의 보상이 언제든지 주어지는 경우에도 충분히 강화학습을 적용할 수 있다.
3. 틱택토 게임과 같이 이산적인 시간 간격으로 분해되지 않는 경우에도 강화학습을 적용할 수 있다.
4. 게임 규칙 외의 사전 정보 없이 학습을 시작한 틱택토 게임의 예와 달리, 효율적인 학습을 위해 필수적일 수 있는 다양한 사전 정보를 강화학습에 포함시킬 수 있다.
5. 강화학습은 학습자가 자신이 취하지 않을 선택에 대해 환경이 어떻게 변할지를 예측할 수 있는 예측 모델 필요로 하지 않으며, 따라서 어떠한 문제에도 강화학습을 적용할 수 있다. 만약 모델이 주어진다면 그 모델을 쉽게 사용하여 문제를 해결하고, 모델이 없더라도 학습이 가능하다.
6. 강화학습은 한 시스템의 높은 수준과 낮은 수준 모두에서 사용 가능하다. 

---
### 강화학습의 초기 역사
강화학습의 초기 역사에는 두 가지 주요한 갈래가 있다.
- 시행착오 학습 : 동물 심리학에서 유래된 것으로, 인공지능의 초기 연구에 만연한 흐름으로서 1980년대 초 강화학습의 부흥을 야기
- 최적 제어의 문제와 그 해결책 : 가치 함수와 동적 프로그래밍을 이용하는 것

그리고 잘 알려지지 않은 세 번째 갈래로서, 시간차 방법에 관한 것이 있다. 이후 1980년대 후반에 세 갈래가 모여 현대 강화학습 분야를 만들어 내었다.

#### 최적 제어
1. 최적 제어
    - 동역학 시스템의 시간에 따른 결과를 측정하고 그 측정값을 최대 혹은 최소화하는 제어기를 설계하기 위해 1950년대 후반에 등장
    - 최적 제어 접근법 중 하나로 리처드 벨만을 비롯한 학자들에 의해 벨만 방정식이 개발
       - 해밀턴(Hamilton)과 자코비(Jacobi)의 19세기 초 이론을 확장하는 방법을 사용
       - 동역학 시스템의 상태와 가치 함수, 즉 '최적 이득 함수(Optimal return function)'를 사용
       - 이를 이용해 최적 제어 문제를 해결하는 방법이 '동적 프로그래밍'으로 알려짐
    - 벨만은 또한 마르코프 결정 과정(MDP)이라고 알려진 최적 제어 문제를 이산 확률론적으로 다룬 새로운 시각도 제시함
    - 로널드 하워드는 MDP에 대한 '정책 반복' 방법을 고안함
2. 동적 프로그래밍
    - 일반적인 확률론적 최적 제어 문제를 푸는 데 있어 실현 가능한 유일한 방법으로 알려짐
    - 차원의 저주 문제는 여전히 존재하나, 다른 방법보다 훨씬 효율적이고 폭 넓게 적용 가능한 방법
       - 차원의 저주 : 상태 변수의 개수에 따라 계산량이 기하급수적으로 증가하는 문제
3. 최적 제어의 두 연결 고리
    - 최적 제어는 한편으로는 동적 프로그래밍과, 다른 한편으로는 학습과 연결 고리가 있으나, 두 연결 고리의 구분은 명확하지 않다.
    - 일반적으로 각 연결 고리는 내재된 방법과 목표의 차이를 통해 구별 가능하다.
    - 벨만과 드레퓌스, 위튼, 웨어보스 등의 연구는 동적 프로그래밍을 바라보는 여러 관점을 제시하였다.
    - 1989년 크리스 왓킨스의 연구 이후 동적 프로그래밍 방법과 온라인 학습의 완전한 결합이 등장하였다.
    - '신경 동역학 프로그래밍', '근사적 동적 프로그래밍' 등의 용어도 사용되며, 이러한 접근법은 강화학습을 이용해 동적 프로그래밍의 오랜 단점을 극복하는 데 관심을 둔다.
4. 강화학습과 최적 제어
    - 강화학습의 문제는 최적 제어의 문제, 특히 MDP 형식을 갖는 확률론적 최적 제어의 문제와 밀접한 관계가 있다.
    - 따라서 동적 프로그래밍과 같은 최적 제어의 해법은 강화학습의 방법에 해당한다.

#### 시행착오 학습
1. 손다이크, 효과의 법칙(Law of effect)
   - 학습의 원리로서 시행착오 학습의 본질을 최초로 간결하게 표현
   - "다른 조건이 동일할 경우, 동일한 상황에 대한 여러 반응 중 만족을 동반하거나 곧바로 만족을 유발하는 반응이 그 상황과 더욱 확고하게 연결되어 있으며, 따라서 그 상황이 다시 발생하면 그 반응 역시 다시 발현될 가능성이 높다. 반대로 불편함을 동반하거나 유발하는 반응은 주어진 상황과 더 약하게 연결되므로 동일 상황 발생 시 그 반응이 발현될 확률이 낮아진다. 즉, `만족이 클수록 상황과 반응의 연결 고리는 강화되고, 붏편함이 클수록 그 연결 고리는 약화된다.`"
   - 이는 행동 선택의 경향을 강화하는 사건의 효과를 나타내어 '효과의 법칙'으로 명명되었다.
   - 이 이론은 학습 이론가들 사이에 상당한 논란을 야기했음에도 불구하고 행동 기저의 기본 원리로 폭넓게 받아들여진다.
   - 여기서 사용된 '강화'라는 용어는 이후 활발히 사용되었으며, 대표적인 예로 파블로브의 조건 반사 이론을 들 수 있다.
2. 컴퓨터에 시행착오 학습을 적용하려는 다양한 발상
   - 인공지능에 대한 초기 발상에서 등장한 시행착오 학습
      - 엘런 튜링의 '기쁨-고통 시스템(Pleasure-pain system)'
   - 시행착오 학습을 보이는 전기 기계식 장치들의 개발
      - 토마스 로스의 '미로 찾기 기계'
      - 그레이 월터의 '기계식 거북이' : 간단한 형태의 학습 가능
      - 클로드 섀넌의 '테세우스' : 미로 찾기 쥐
   - (시행착오 학습을 포함한) 다양한 형태의 학습을 수행하는 디지털 컴퓨터 프로그램
      - 필리와 클락을 필두로 시행착오 학습과 지도학습의 개념이 혼용되면서 1960~1970년대까지는 시행착오 학습 연구가 제대로 이루어지지 않았다.
      - 당시 연구 중 '진정한' 강화학습에 대한 연구가 이루어진 몇몇 예외가 존재한다.
         - 존 안드라아, STeLLA : 숨겨진 상태에 관한 문제를 다루기 위한 '내적 독백'을 도입, 후기 연구에서 시행착오 학습에 관한 내용을 포함함
         - 도널드 미치, MENACE : 틱택토 게임을 배우는 간단한 시행착오 학습 시스템
         - 미치와 체임버스, GLEE와 BOXES : GLEE라는 이름의 틱택토 강화학습 학습자와 BOXES라는 이름의 강화학습 제어기를 개발, 움직이는 카트 위에 경첩으로 연결된 막대가 쓰러지지 않고 균형을 유지하도록 하는 데 BOXES를 적용
         - 위트로, 굽타, 마이트라의 '선택적 부트스트랩 적용' : 위드로와 호프의 최소 평균 제곱 알고리즘을 수정하여 훈련 예제 대신 성공과 실패의 신호로부터 배우는 강화학습의 규칙을 작성, '교사로부터 배우는 대신 비평자로부터 배우는 것'으로 설명함
3. 학습 로봇(Learning automata)에 대한 연구
   - 시행착오 분야에 더 직접적인 영향을 주고, 현대 강화학습 연구로 이어졌다.
   - 비연합적(Nonassociative)이고 순수하게 선택적인 학습 문제를 풀기 위한 연구
   - 레버가 여러 개인 슬롯 머신(단일 선택)과 유사하여 다중 선택(k-armed abndit)으로 알려졌다.
   - 1960년대 체틀린과 동료들이 수행한 연구에서 비롯되었으며, 등장 이후 공학 분야에서 폭넓게 발전하였다.
      - 확률론적 학습 로봇 : 보상 신호에 기반하여 행동의 확률을 갱신
      - 하스와 차나코우, Alopex알고리즘 : 행동과 강화 사이의 상관관계를 감지하는 확률론적 방법
4. 통계적 학습 이론
   - 심리학에서 발전하여 경제학 분야의 연구에 활용되어 경제학 분야의 강화 학습 연구라는 한 갈래를 형성
   - 1973년 부시와 모스텔러가 학습 이론을 일군의 전통적 경제학 모델에 적용하면서 시작됨
5. 존 홀랜드의 '선택 원리에 기반한 적응 시스템의 일반 이론'
   - 1986년 연합과 가치 함수를 포함하는 진정한 강화학습 시스템인 `분류 시스템`을 제시
   - 홀랜드의 분류 시스템의 핵심 요소
      - 양동이 집합 알고리즘 : 신뢰 할당을 위한 알고리즘
      - 유전자 알고리즘 : 유용한 속성을 선택적으로 진화시키는 방법
6. 해리 클로프의 연구
   - 인공지능 분야에서 시행착오에 기반한 강화학습 방법을 부활시키는 데 가장 큰 기여
   - 학습에 대한 연구가 거의 배타적으로 지도학습에 초점을 맞출 경우 적응하는 행동의 본질적 측면을 잃어버리게 된다는 사실을 깨달았다.
   - 이때 잃어버리는 본질적 측면은 행동의 쾌락 지향적 측면, 환경으로부터 어떤 결과를 이끌어내려고 하는 욕망, 환경이 원하는 목표에 다가가거나 원치 않는 상태로부터 벗어나게끔 환경을 제어하려는 욕망 등이다.
         
#### 시간차 학습
1. 개요
   - 강화학습의 역사에 있어 세 번째 갈래에 해당
   - 특정 값을 시간에 따라 연속적으로 추정하고, 연속한 두 추정값 사이의 차이로부터 학습 방법을 도출한다는 점에서 다른 학습 방법과 구별된다.
   - 상기 두 학습 방법에 비해 연구도 적게 되었고 명확성도 떨어지나, 강화학습 분야에서 매우 중요한 역할을 하였다.
2. 민스키와 사무엘의 연구
   - 민스키의 연구
     - 동물학습 심리학의 원리를 인공학습 시스템에 적용한 최초의 연구자
     - `2차 강화자`(음식이나 고통 등과 같은 주요 강화자와 짝을 이루는 자극으로, 주요 강화자와 동일한 강화 특성을 가짐)라는 개념으로부터 시간차 학습을 시작
   - 아서 사무엘의 연구
     - 클로드 섀넌의 연구(컴퓨터가 체스 게임을 할 때 가치 함수를 사용하도록 프로그래밍하고, 컴퓨터는 가치 함수를 실시간으로 수정하면서 자신의 실력을 향상시킬 수 있다)에 영향을 받음
   - 민스키는 1961년 '단계들'이라는 논문을 통해 사무엘의 연구를 폭넓게 다루고, 사무엘의 연구와 2차 강화 이론의 연관성을 자연적 관점 및 인공적 관점에서 제시함
3. 클로프와 서튼의 연구
   - 클로프의 연구
      - 1972년 시간차 학습의 중요한 요소와 함께 시행착오 학습을 도입
      - 모든 구성 요소가 모든 입력을 강화의 측면에서 인식하는 '일반화된 강화학습'의 개념을 개발
      - 일반화된 강화학습을 시행착오 학습과 연결하고, 동물학습 심리학의 방대한 경험적 데이터와 관련지어 설명함
   - 서튼의 연구
      - 클로프의 생각, 특히 동물학습 이론과의 연결성에 대한 개념을 한층 더 발전시키면서, 그 과정에서 '연속적인 예측의 변화로부터 도출되는 학습 규칙'을 설명
      - 서튼과 바트로는 상기의 개념을 다듬고 시간차 학습에 기반한 '고전적 조건화'의 심리학적 모델을 개발함
4. 바르토의 '행동자-비평자 구조'
   - 시간차 학습에 대한 상기 초기 연구들을 바탕으로 시간차 학습과 시행착오 학습을 결함하여 개발한 방법
   - 1986년 앤더슨의 논문에서 역전파 신경망에 활용됨
5. 이안 위튼의 TD(0)
   - 시간차 학습 규칙을 최초로 다룬 논문
   - TD(0)은 표 형태로 된 방법으로서, MDP를 풀기 위한 적응 제어기의 일부로 활용됨
   - 안드라아가 STeLLA를 이용해 수행한 초기의 실험과 그 밖의 시행착오 학습 시스템을 계승함
   - 강화학습의 주요 갈래인 시행착오 학습과 최적 제어를 모두 아우르며, 동시에 시간차 학습의 초기 발전에 기여함
6. 크리스 왓킨스의 Q 학습(1989년)
   - 시간차와 최적 제어의 두 갈래를 완전히 함께 도입
   - 이전까지 세 개의 갈래로 나누어진 강화학습 연구를 확장하고 결합함
   - 이 시기 강화학습 연구가 크게 성장하였으며, 특히 인공 신경망과 인공지능 분야에서 폭넓게 연구가 이루어졌다.
